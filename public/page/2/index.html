<!DOCTYPE html>
<html lang="en-us">
<head>
	<meta name="generator" content="Hugo 0.111.3">
	<meta charset="UTF-8">
	<meta name="viewport" content="width=device-width, initial-scale=1">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<style type=text/css>body{font-family:monospace;}</style>
	<title>Nooblogaurus</title>
	
	
	<link rel="stylesheet" href="/css/style.css">
	
	<link rel="alternate" type="application/rss+xml" href="/index.xml" title="Nooblogaurus">
</head>
<body>
	<header>
	==================<br>
	== <a href="https://noobosaurus-r3x.github.io">Nooblogaurus</a> ==<br>
	==================
	<div style="float: right;">Le Blog de Noobosaurus R3x</div><br>
	<p>
	<nav>
			<a href="/"><b>Start</b></a>.
			
			
			<a href="/posts/"><b>Posts</b></a>.
			
			<a href="/categories/"><b>Categories</b></a>.
			
			<a href="/tags/"><b>Tags</b></a>.
			
	</nav>
	</p>
	
</header>

	
	<main>
		
		
			<article>
	<h1><a href="https://noobosaurus-r3x.github.io/posts/johnzipper/">Johnzipper</a></h1>
	<b><time>18.06.2023 02:11</time></b>
	
	<a href="/tags/tools">tools</a>
	
	<a href="/tags/bash">bash</a>
	
	<div>
		Just a simple bash script to automate the process of cracking a zip file using zip2john and john the ripper. #!/bin/bash # Check if the zip file name is provided as an argument if [ -z &#34;$1&#34; ]; then echo &#34;Please provide the name of the zip file as an argument.&#34; exit 1 fi zip_file=&#34;$1&#34; hash_file=&#34;pass.hash&#34; password_list=&#34;/usr/share/wordlists/rockyou.txt&#34; # Run zip2john to extract the hash zip2john &#34;$zip_file&#34; &gt; &#34;$hash_file&#34; # Check if the hash file was successfully created if [ !
		
			<a href="https://noobosaurus-r3x.github.io/posts/johnzipper/">Read more...</a>
		
	</div>
</article>

		
			<article>
	<h1><a href="https://noobosaurus-r3x.github.io/posts/crawwwler/">Crawwwler</a></h1>
	<b><time>09.06.2023 20:23</time></b>
	
	<a href="/tags/tools">tools</a>
	
	<a href="/tags/bash">bash</a>
	
	<div>
		Web Crawler Bash Script This script is a simple web crawler written in Bash. It takes a URL as an argument, downloads the HTML of the page, extracts all the links, and checks the HTTP status code for each link. It then prints the URL and its HTTP status code, with URLs returning a 200 status code highlighted in green and all others in red. #!/bin/bash usage() { echo &#34;Usage: $0 -u URL&#34; exit 1 } while getopts u: flag do case &#34;${flag}&#34; in u) url=${OPTARG};; *) usage;; esac done if [ -z &#34;$url&#34; ]; then usage fi IMAGE=&#34; +-+-+-+-+-+-+-+-+-+-+-+-+ |c|r|a|w|w|w|l|e|r|.
		
			<a href="https://noobosaurus-r3x.github.io/posts/crawwwler/">Read more...</a>
		
	</div>
</article>

		
		<div>

	<a href="/">Previous Page</a>

2 of 2

</div>

	</main>

	<footer>
	<p>&copy; 2024 <a href="https://noobosaurus-r3x.github.io"><b>Nooblogaurus</b></a>.
	<a href="https://www.youtube.com/@HacktBack"><b>YouTube_HacktBack</b></a>.
	<a href="https://www.youtube.com/@Noobosaurus"><b>YouTube_Noobosaurus</b></a>.
	<a href="https://twitter.com/NoobosaurusR3x"><b>Twitter</b></a>.
	<a href="https://www.linkedin.com/in/noobosaurus-r3x-683783231/"><b>LinkedIn</b></a>.
	<a href="https://github.com/noobosaurus-r3x"><b>Github</b></a>.
	</p>
</footer>

</body>
</html>
